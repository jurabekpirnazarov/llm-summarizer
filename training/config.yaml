model_id: mistralai/Mistral-7B-Instruct-v0.2
output_dir: ./outputs/lora


lora:
r: 16
alpha: 32
dropout: 0.05
target_modules: ["q_proj", "v_proj"]


qlora:
use_qlora: true
load_in_4bit: true
bnb_4bit_use_double_quant: true
bnb_4bit_quant_type: nf4
bnb_4bit_compute_dtype: float16


train:
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 2.0e-4
num_train_epochs: 2
max_seq_length: 2048
logging_steps: 50
save_steps: 500
eval_steps: 500
warmup_ratio: 0.03
lr_scheduler_type: cosine
bf16: true

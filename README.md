# ğŸ“ LLM Text Summarizer (Multiâ€‘Style Output)

This project is a **productionâ€‘like, portfolioâ€‘ready LLM app** that generates summaries in different styles â€” **formal**, **casual**, and **bulletâ€‘point** â€” via a REST API. It includes LoRA/QLoRA fineâ€‘tuning, evaluation scripts, tests, Docker support, and CI hooks.

![Python](https://img.shields.io/badge/Python-3.10+-blue)
![FastAPI](https://img.shields.io/badge/Web-FastAPI-green)
![Transformers](https://img.shields.io/badge/HF-Transformers-orange)
![License: MIT](https://img.shields.io/badge/License-MIT-yellow)

---

## âœ¨ Features

* Multiâ€‘style summaries (formal, casual, bulletâ€‘point)
* Controllable prompts for different styles
* LoRA/QLoRA fineâ€‘tuning on a custom dataset
* FastAPI inference server with REST endpoint
* Offline evaluation (ROUGE, BERTScore)
* Dockerized + GitHub Actions template

---

## ğŸ“‚ Repository Structure

```
llm-summarizer/
â”œâ”€ app/              # FastAPI app, prompt templates, schemas
â”œâ”€ training/         # LoRA/QLoRA fineâ€‘tuning scripts
â”œâ”€ eval/             # Evaluation scripts
â”œâ”€ tests/            # Unit & API tests
â”œâ”€ docker/           # Dockerfile
â”œâ”€ scripts/          # Utility scripts (adapter export)
â”œâ”€ data/             # Dataset samples & format docs
â”œâ”€ requirements.txt
â””â”€ README.md
```

---

## ğŸš€ Quickstart

### 1. Environment Setup

```bash
python -m venv .venv && source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
```

### 2. Run the API

```bash
export MODEL_ID=mistralai/Mistral-7B-Instruct-v0.2
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

### 3. Test the Endpoint

```bash
curl -X POST http://localhost:8000/summarize \
  -H "Content-Type: application/json" \
  -d '{"text":"Your long article text...","style":"formal","max_tokens":200}'
```

Example response:

```json
{
  "style": "formal",
  "summary": "This article explains..."
}
```

---

## ğŸ“Š Data Format

Dataset format (JSONL):

```jsonl
{"text": "Long text...", "summary_formal": "...", "summary_casual": "...", "summary_bullet": ["â€¢ ...", "â€¢ ..."]}
```

See `data/README.md` for details.

---

## ğŸ§© Fineâ€‘Tuning

* **LoRA** and **QLoRA** supported via HuggingFace PEFT.
* Configurable with `training/config.yaml`.
* Example:

```bash
python training/train_lora.py --data data/samples.jsonl --style formal
```

---

## ğŸ“ Evaluation

Evaluate summaries with ROUGE and BERTScore:

```bash
python eval/eval.py --pred eval/sample_eval.jsonl
```

---

## ğŸ§ª Tests

Run tests with pytest:

```bash
pytest tests/
```

---

## ğŸ³ Docker

Build and run container:

```bash
docker build -t llm-summarizer -f docker/Dockerfile .
docker run -p 8000:8000 llm-summarizer
```

---

## ğŸ“ˆ Roadmap / Stretch Goals

* [ ] Add Streamlit demo UI
* [ ] Push fineâ€‘tuned adapters to HuggingFace Hub
* [ ] Add CI/CD via GitHub Actions
* [ ] Multilingual summarization

---

## ğŸ“œ License

MIT License â€” free to use and modify.

---

## ğŸ“£ Credits

Built with â¤ï¸ using:

* [HuggingFace Transformers](https://huggingface.co/transformers/)
* [PEFT / LoRA](https://huggingface.co/docs/peft/)
* [FastAPI](https://fastapi.tiangolo.com/)
